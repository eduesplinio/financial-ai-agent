/**
 * Response Validation
 *
 * This module provides tools to validate and evaluate the quality of RAG responses
 * for financial questions, ensuring they meet quality standards.
 */

import { LLMService } from '../llm/llm-service';
import { createFactCheckPrompt } from '../llm/prompt-engineering';
import { KnowledgeDocument } from '@/shared/src/types';

export interface ValidatedResponse {
  content: string;
  validationPassed: boolean;
  quality: {
    factualConsistency: number;
    completeness: number;
    citationQuality: number;
    overall: number;
  };
  issues?: string[] | undefined;
}

export interface ValidationOptions {
  strictness: 'low' | 'medium' | 'high';
  validateCitations: boolean;
  validateFactualConsistency: boolean;
  minimumFactualScore: number; // 0-10
  minimumCompletenessScore: number; // 0-10
}

/**
 * The ResponseValidator class provides methods to evaluate and validate
 * responses generated by the RAG system.
 */
export class ResponseValidator {
  private llmService: LLMService;

  constructor(llmService: LLMService) {
    this.llmService = llmService;
  }

  /**
   * Validates a response based on provided source documents and query
   *
   * @param response The response content to validate
   * @param query The original query that generated the response
   * @param sourceDocuments The source documents used to generate the response
   * @param options Validation options
   */
  async validateResponse(
    response: string,
    query: string,
    sourceDocuments: KnowledgeDocument[],
    options: ValidationOptions = {
      strictness: 'medium',
      validateCitations: true,
      validateFactualConsistency: true,
      minimumFactualScore: 7,
      minimumCompletenessScore: 6,
    }
  ): Promise<ValidatedResponse> {
    // Combine source documents into context for validation
    const context = sourceDocuments
      .map(
        (doc, i) =>
          `Documento #${i + 1}\nTítulo: ${doc.title}\nFonte: ${doc.source}\nConteúdo:\n${doc.content}\n---`
      )
      .join('\n\n');

    // Use fact-checking prompt to evaluate response
    const factCheckPrompt = createFactCheckPrompt(query, response, context);
    const factCheckResponse =
      await this.llmService.generateResponse(factCheckPrompt);

    // Parse validation results
    const factualScoreMatch = factCheckResponse.content.match(
      /Precisão factual: (\d+)/i
    );
    const factualsCorrectMatch = factCheckResponse.content.match(
      /Afirmações corretas: (.+?)(?=\n|$)/s
    );
    const factualsIncorrectMatch = factCheckResponse.content.match(
      /Afirmações incorretas: (.+?)(?=\n|$)/s
    );
    const citationsMatch = factCheckResponse.content.match(
      /Citações adequadas: (\w+)/i
    );
    const completenessMatch = factCheckResponse.content.match(
      /Completude da resposta: (\d+)/i
    );

    // Extract scores
    const factualScore =
      factualScoreMatch && factualScoreMatch[1]
        ? Math.min(10, Math.max(0, parseInt(factualScoreMatch[1], 10)))
        : 5;

    const citationQuality =
      citationsMatch && citationsMatch[1]
        ? citationsMatch[1].toLowerCase() === 'sim'
          ? 1
          : citationsMatch[1].toLowerCase() === 'parcial'
            ? 0.5
            : 0
        : 0.5;

    const completenessScore =
      completenessMatch && completenessMatch[1]
        ? Math.min(10, Math.max(0, parseInt(completenessMatch[1], 10)))
        : 5;

    // Identify issues
    const issues: string[] = [];

    if (
      options.validateFactualConsistency &&
      factualScore < options.minimumFactualScore
    ) {
      issues.push(
        `Precisão factual abaixo do mínimo: ${factualScore}/${options.minimumFactualScore}`
      );

      // Add details about incorrect statements if available
      if (
        factualsIncorrectMatch &&
        factualsIncorrectMatch[1] &&
        factualsIncorrectMatch[1].trim() !== 'Nenhuma'
      ) {
        issues.push(
          `Informações incorretas: ${factualsIncorrectMatch[1].trim()}`
        );
      }
    }

    if (completenessScore < options.minimumCompletenessScore) {
      issues.push(
        `Completude abaixo do mínimo: ${completenessScore}/${options.minimumCompletenessScore}`
      );
    }

    if (options.validateCitations && citationQuality < 0.5) {
      issues.push('Citações inadequadas ou ausentes');
    }

    // Calculate overall quality score (0-1)
    const factualWeight = 0.6;
    const completenessWeight = 0.3;
    const citationWeight = 0.1;

    const normalizedFactualScore = factualScore / 10;
    const normalizedCompletenessScore = completenessScore / 10;

    const overallScore =
      normalizedFactualScore * factualWeight +
      normalizedCompletenessScore * completenessWeight +
      citationQuality * citationWeight;

    // Determine if validation passes based on strictness
    const passThresholds = {
      low: 0.5,
      medium: 0.7,
      high: 0.85,
    };

    const validationPassed = overallScore >= passThresholds[options.strictness];

    return {
      content: response,
      validationPassed,
      quality: {
        factualConsistency: normalizedFactualScore,
        completeness: normalizedCompletenessScore,
        citationQuality,
        overall: overallScore,
      },
      issues: issues.length > 0 ? issues : undefined,
    };
  }

  /**
   * Perform a simpler but faster response quality check
   * This uses heuristics rather than LLM evaluation
   */
  evaluateResponseQuality(response: string, citationCount?: number): number {
    // Simple heuristic-based quality assessment
    const metrics: Record<string, number> = {};

    // Length-based quality (0-0.2)
    // Too short responses are often not helpful
    const length = response.length;
    metrics.length = Math.min(0.2, (length / 2000) * 0.2);

    // Structure quality (0-0.2)
    // Good responses often have multiple paragraphs, section breaks, etc.
    const paragraphs = response.split('\n\n').filter(p => p.trim().length > 0);
    metrics.structure = Math.min(0.2, (paragraphs.length / 5) * 0.2);

    // Citation quality (0-0.4)
    // If citation count provided, use it, otherwise try to detect them
    let citations = citationCount;
    if (citations === undefined) {
      const citationMatches = response.match(/\[\d+\]/g);
      citations = citationMatches ? citationMatches.length : 0;
    }
    metrics.citations = Math.min(0.4, (citations / 5) * 0.4);

    // Content variety (0-0.2)
    // Look for various elements like numbers, bullet points, questions
    let varietyScore = 0;
    if (/\d+[.,]\d+/.test(response)) varietyScore += 0.05; // Contains numbers with decimals
    if (/\n[-*•] /.test(response)) varietyScore += 0.05; // Contains bullet points
    if (/\?/.test(response)) varietyScore += 0.05; // Contains questions
    if (/\d{4}/.test(response)) varietyScore += 0.05; // Contains years
    metrics.variety = varietyScore;

    // Calculate overall score
    const overall = Object.values(metrics).reduce(
      (sum, score) => sum + score,
      0
    );

    return Math.min(1, Math.max(0, overall));
  }
}
